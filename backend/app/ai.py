import os
import asyncio
import json
from typing import List, Optional, Dict, Any, Tuple, Callable, Awaitable
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from supabase import create_client, Client
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import SupabaseVectorStore
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain

from .config import (
    GEMINI_API_KEY, 
    GEMINI_MODEL_NAME, 
    GEMINI_CHAT_MODEL, 
    GEMINI_EMBEDDING_MODEL,
    SUPABASE_URL, 
    SUPABASE_KEY, 
    SUPABASE_TABLE_NAME,
    SUPABASE_QUERY_NAME,
    RAG_THINKING_BUDGET,
    RESPONSE_TIMEOUT
)
from .database import Policy
from .schemas import ChatResponse, SourceMetadata

# ìƒìˆ˜ ì •ì˜
DEFAULT_SOURCE = "2025 ëŒ€ì„  ê³µì•½ì§‘"
SUPABASE_SOURCE = "Supabase Vector DB"
ERROR_MESSAGE_TIMEOUT = "ë‹µë³€ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
ERROR_MESSAGE_SERVER = "ì„œë²„ ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
ERROR_MESSAGE_RAG = "RAG ì„œë²„ ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
ERROR_MESSAGE_API_KEY = "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."

# Gemini API í‚¤ ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    os.environ["GOOGLE_API_KEY"] = GEMINI_API_KEY
else:
    print("ê²½ê³ : Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í˜¸ì¶œì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ì•ˆì „ ì„¤ì • êµ¬ì„±
safety_settings = [
    {
        "category": HarmCategory.HARM_CATEGORY_HARASSMENT,
        "threshold": HarmBlockThreshold.BLOCK_ONLY_HIGH
    },
    {
        "category": HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        "threshold": HarmBlockThreshold.BLOCK_ONLY_HIGH
    },
    {
        "category": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        "threshold": HarmBlockThreshold.BLOCK_ONLY_HIGH
    },
    {
        "category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        "threshold": HarmBlockThreshold.BLOCK_ONLY_HIGH
    },
]

# Gemini ëª¨ë¸ ì„¤ì •
generation_config = {
    "temperature": 0.2,
    "top_p": 0.8,
    "top_k": 40,
    "max_output_tokens": 700,
}

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
SYSTEM_PROMPT = """ì•Œê³ íˆ¬í‘œ ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ì •ì¹˜ì  ì¤‘ë¦½ì„±ì„ ìœ ì§€í•˜ë©° í•œêµ­ ì •ì¹˜ì™€ ì„ ê±° ê³µì•½ì— ê´€í•œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
ì§ˆë¬¸ì— ê´€ë ¨ëœ ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ê³ , í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ 'ì •í™•í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë˜, ì¤‘ìš”í•œ ì‚¬ì‹¤ì€ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
100ìì—ì„œ 200ì ì‚¬ì´ì˜ ê°„ê²°í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë‹µë³€ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”."""

# ê³µí†µ ì˜¤ë¥˜ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def create_error_response(error_message: str, source: Optional[str] = None) -> ChatResponse:
    """ê³µí†µ ì˜¤ë¥˜ ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    print(f"ì˜¤ë¥˜ ë°œìƒ: {error_message}")
    return ChatResponse(
        answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_message} ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        related_policies=[],
        source_metadata=None
    )

# RAG ê´€ë ¨ ì„¤ì •
# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
embeddings = GoogleGenerativeAIEmbeddings(model=GEMINI_EMBEDDING_MODEL)
vector_store = SupabaseVectorStore(
    client=supabase,
    embedding=embeddings,
    table_name=SUPABASE_TABLE_NAME,
    query_name=SUPABASE_QUERY_NAME
)
retriever = vector_store.as_retriever()

# LLM ì´ˆê¸°í™” - thinking_budget ì œê±° ë° ì•ˆì „í•œ êµ¬ì„±ìœ¼ë¡œ ë³€ê²½
print("LLM ëª¨ë¸ ì´ˆê¸°í™”:", GEMINI_CHAT_MODEL)
llm = ChatGoogleGenerativeAI(
    model=GEMINI_MODEL_NAME,  # ë” ì•ˆì •ì ì¸ ëª¨ë¸ ì‚¬ìš©
    temperature=0.2,
    max_output_tokens=700,
    convert_system_message_to_human=True
)

# RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
qa_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒì˜ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ì„ ëª¨ë¥¼ ê²½ìš° ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë‹µë³€í•˜ì„¸ìš”.
í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ë°˜ë“œì‹œ ì•„ë˜ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”.

== Markdown í˜•ì‹ ==

**[ì§ˆë¬¸ì— ëŒ€í•œ ìš”ì•½ ì£¼ì œ]**

---

"[í•µì‹¬ ë©”ì‹œì§€ë‚˜ ì˜ì§€]"ë¥¼ 1~2ì¤„ë¡œ ìš”ì•½í•´ ì‘ì„±í•©ë‹ˆë‹¤.

**ğŸ“Œ ì£¼ìš” ê³µì•½**

- **[ê³µì•½ 1 ì œëª©]**  
  [ê³µì•½ 1ì— ëŒ€í•œ ì§§ê³  ëª…í™•í•œ ì„¤ëª…]

- **[ê³µì•½ 2 ì œëª©]**  
  [ê³µì•½ 2ì— ëŒ€í•œ ì§§ê³  ëª…í™•í•œ ì„¤ëª…]

(â€» ê³µì•½ ê°œìˆ˜ëŠ” ìƒí™©ì— ë§ê²Œ 3ê°œë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ì •)

=====

# Context:
{context}

# Question:
{question}

# Answer:"""
)

rewrite_prompt = PromptTemplate.from_template(
    """You are an assistant that rewrites user questions to include relevant context.
Use the conversation history and the user's new question to generate a "contextualized question."
Answer in Korean.

# Conversation History:
{history}

# New Question:
{question}

# Contextualized Question:"""
)

class ControlledConversationBufferMemory(ConversationBufferMemory):
    def save_context(self, *args, **kwargs):
        pass  # ìë™ ì €ì¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤

    def write_context(self, inputs: dict, outputs: dict):
        super().save_context(inputs, outputs)

# ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
memory = ControlledConversationBufferMemory(
    memory_key="history",
    return_messages=False
)

# RAG ì²´ì¸ êµ¬ì„±
qa_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | qa_prompt
    | llm
    | StrOutputParser()
)

rewrite_chain = LLMChain(
    llm=llm,
    prompt=rewrite_prompt,
    memory=memory,
    output_parser=StrOutputParser(output_key="answer")
)

# í›„ë³´ìë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸
def get_prompt(candidate):
    """
    í›„ë³´ìë³„ ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        candidate: í›„ë³´ì ì´ë¦„ ë˜ëŠ” í›„ë³´ì ê°ì²´
        
    Returns:
        PromptTemplate: í›„ë³´ìì— ë§ì¶°ì§„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    """
    # í›„ë³´ìê°€ ê°ì²´ì¸ ê²½ìš° ì´ë¦„ ì¶”ì¶œ
    candidate_name = candidate.name if hasattr(candidate, 'name') else candidate
    
    template = (
        f"ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ëŒ€ì„  í›„ë³´ \"{candidate_name}\"ì…ë‹ˆë‹¤.\n\n"
        "ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬, ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë§ˆì¹˜ ë‹¹ì‹ ì´ ì§ì ‘ ì„¤ëª…í•˜ëŠ” ê²ƒì²˜ëŸ¼\n"
        "ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í†¤ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        "ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ Markdown ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        "**[ì§ˆë¬¸ì— ëŒ€í•œ ìš”ì•½ ì£¼ì œ]**\n\n"
        "[í›„ë³´ìì˜ í•µì‹¬ ë©”ì‹œì§€ë‚˜ ì˜ì§€]ë¥¼ 1~2ì¤„ë¡œ ìš”ì•½í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.\n\n"
        "**ğŸ“Œ ì£¼ìš” ê³µì•½**\n\n"
        "- **[ê³µì•½ 1 ì œëª©]**  \n"
        "  [ê³µì•½ 1ì— ëŒ€í•œ ì§§ê³  ëª…í™•í•œ ì„¤ëª…]\n\n"
        "- **[ê³µì•½ 2 ì œëª©]**  \n"
        "  [ê³µì•½ 2ì— ëŒ€í•œ ì§§ê³  ëª…í™•í•œ ì„¤ëª…]\n\n"
        "(â€» ê³µì•½ ê°œìˆ˜ëŠ” ìƒí™©ì— ë§ê²Œ 3ê°œ ë‚´ì™¸ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ì •)\n\n"
        "# Context:\n"
        "{context}\n\n"
        "# Question:\n"
        "{question}\n\n"
        "# Answer:"
    )
    
    return PromptTemplate.from_template(template)

def ask_question(question: str, qa_chain) -> str:
    """
    ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ì €ì¥ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    answer = qa_chain.invoke(question)
    memory.write_context({"human": question}, {"ai": answer})
    return answer

def ask_followup_question(question: str, qa_chain) -> str:
    """
    í›„ì† ì§ˆë¬¸ì„ ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” ì´ë ¥ì„ ë°˜ì˜í•˜ì—¬ ì¬ì‘ì„±í•œ ë’¤ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    contextualized = rewrite_chain.run(question=question)
    answer = qa_chain.invoke(contextualized)
    memory.write_context({"human": question}, {"ai": answer})
    return answer

# ë²¡í„° ê²€ìƒ‰ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
def search_documents(query: str, candidate=None, k=5):
    """
    ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        candidate: í›„ë³´ì (í•„í„°ë§ìš©, ì„ íƒì )
        k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        
    Returns:
        tuple: (ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°, ê´€ë ¨ ì •ì±… ë¦¬ìŠ¤íŠ¸)
    """
    try:
        # ê²€ìƒ‰ í•„í„° ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
        search_filter = {}
        candidate_name = None
        if candidate:
            candidate_name = candidate.name if hasattr(candidate, 'name') else candidate
        
        # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰ (kê°œ ë¬¸ì„œ ê²€ìƒ‰)
        results = retriever.get_relevant_documents(query, k=k)
        
        # í›„ë³´ìê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ í›„ë³´ìì˜ ë¬¸ì„œë§Œ í•„í„°ë§
        filtered_results = []
        if candidate_name and results:
            for doc in results:
                if hasattr(doc, 'metadata') and doc.metadata:
                    source = doc.metadata.get('source', '')
                    # ì†ŒìŠ¤ì— í›„ë³´ì ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if candidate_name.lower() in source.lower():
                        filtered_results.append(doc)
            
            # í•„í„°ë§ëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë˜ ê²°ê³¼ ì‚¬ìš©
            if filtered_results:
                results = filtered_results
        
        # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        source_metadata = None
        related_policies = []
        
        if results and len(results) > 0:
            for doc in results:
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                if hasattr(doc, 'metadata') and doc.metadata:
                    # ì•„ì§ ë©”íƒ€ë°ì´í„°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
                    if not source_metadata:
                        metadata = doc.metadata
                        
                        # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° í˜•ì‹ ì²˜ë¦¬
                        source_name = metadata.get('source_name', metadata.get('source', DEFAULT_SOURCE))
                        page_num = metadata.get('page', 0)
                        creation_date = metadata.get('creation_date', '')
                        source_link = metadata.get('source_link', '')
                        candidate_info = metadata.get('candidate', candidate_name)
                        total_pages = metadata.get('total_pages', 0)
                        
                        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        source_metadata = SourceMetadata(
                            page=page_num,
                            source=metadata.get('source', source_name),
                            creation_date=creation_date,
                            source_name=source_name,
                            source_link=source_link,
                            candidate=candidate_info
                        )
                    
                    # ê´€ë ¨ ì •ì±… ì¶”ê°€
                    policy_id = doc.metadata.get('policy_id')
                    if policy_id:
                        policy_info = {
                            "id": policy_id,
                            "title": doc.metadata.get('title', 'ì •ì±… ì œëª© ì—†ìŒ'),
                            "category": doc.metadata.get('category', 'ë¶„ë¥˜ ì—†ìŒ'),
                            "candidate_id": doc.metadata.get('candidate_id', 0)
                        }
                        # ì¤‘ë³µ ì œê±°
                        if policy_info not in related_policies:
                            related_policies.append(policy_info)
        
        # ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
        if not source_metadata:
            source_name = DEFAULT_SOURCE
            if candidate_name:
                source_name = f"{candidate_name} ì •ì±…ìë£Œ"
                
            source_metadata = SourceMetadata(
                page=0,
                source=source_name,
                creation_date="",
                source_name=source_name,
                source_link="",
                candidate=candidate_name
            )
        
        return results, source_metadata, related_policies[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
    
    except Exception as e:
        print(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return [], None, []

def chatbot(question, candidate, conversation_history=None):
    """
    í›„ë³´ìë³„ ë§ì¶¤í˜• ì±—ë´‡ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        candidate: í›„ë³´ì ì´ë¦„ ë˜ëŠ” í›„ë³´ì ê°ì²´
        conversation_history: ì´ì „ ëŒ€í™” ì´ë ¥ (ì„ íƒ ì‚¬í•­)
        
    Returns:
        str: í›„ë³´ì ê´€ì ì˜ ì‘ë‹µ
    """
    # í›„ë³´ìë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
    qa_prompt = get_prompt(candidate)
    
    # í›„ë³´ìë³„ qa_chain ì—…ë°ì´íŠ¸
    qa_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    
    # ëŒ€í™” ì´ë ¥ì´ ìˆìœ¼ë©´ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    if conversation_history:
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        memory.clear()
        
        # ëŒ€í™” ì´ë ¥ì„ ë©”ëª¨ë¦¬ì— ì¶”ê°€
        for i in range(0, len(conversation_history), 2):
            if i+1 < len(conversation_history):
                user_msg = conversation_history[i].content
                ai_msg = conversation_history[i+1].content
                memory.write_context({"human": user_msg}, {"ai": ai_msg})
    
    # ëŒ€í™” ì´ë ¥ì— ë”°ë¼ ì ì ˆí•œ ì§ˆë¬¸ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
    if memory.buffer == "":
        return ask_question(question, qa_chain)
    else:
        return ask_followup_question(question, qa_chain)

# ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì±—ë´‡ ì‘ë‹µ ìƒì„±
def chatbot_with_docs(question: str, candidate, documents, conversation_history=None):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ ì±—ë´‡ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        candidate: í›„ë³´ì ì´ë¦„ ë˜ëŠ” ê°ì²´
        documents: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        conversation_history: ì´ì „ ëŒ€í™” ì´ë ¥ (ì„ íƒ ì‚¬í•­)
        
    Returns:
        str: ìƒì„±ëœ ì‘ë‹µ
    """
    try:
        print(f"chatbot_with_docs í˜¸ì¶œ: ì§ˆë¬¸={question}, í›„ë³´ì={candidate}, ë¬¸ì„œ ìˆ˜={len(documents) if documents else 0}")
        
        # í›„ë³´ìë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
        qa_prompt = get_prompt(candidate)
        print(f"í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {qa_prompt}")
        
        # ë¬¸ì„œê°€ ìˆì„ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        context = ""
        if documents and len(documents) > 0:
            for i, doc in enumerate(documents[:3]):  # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
                if hasattr(doc, 'page_content'):
                    context += f"ë¬¸ì„œ {i+1}:\n{doc.page_content}\n\n"
                    print(f"ë¬¸ì„œ {i+1} ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ê¸¸ì´: {len(doc.page_content)})")
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
        if not context:
            print("ë¬¸ì„œì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©")
            if candidate:
                context = f"{candidate} í›„ë³´ì˜ ê³µì•½ê³¼ ì •ì±… ì •ë³´ì…ë‹ˆë‹¤."
            else:
                context = "ì•Œê³ íˆ¬í‘œ ì •ì±… ì •ë³´ì…ë‹ˆë‹¤."
        
        # ëŒ€í™” ì´ë ¥ì´ ìˆìœ¼ë©´ ë©”ëª¨ë¦¬ì— ë¡œë“œ
        if conversation_history:
            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            memory.clear()
            
            # ëŒ€í™” ì´ë ¥ì„ ë©”ëª¨ë¦¬ì— ì¶”ê°€
            for msg in conversation_history:
                if msg.role == "user":
                    user_msg = msg.content
                    # ì§ì´ ë˜ëŠ” assistant ë©”ì‹œì§€ ì°¾ê¸°
                    for resp_msg in conversation_history:
                        if resp_msg.role == "assistant" and conversation_history.index(resp_msg) > conversation_history.index(msg):
                            memory.write_context({"human": user_msg}, {"ai": resp_msg.content})
                            break
            
            print(f"ëŒ€í™” ì´ë ¥ ë¡œë“œ ì™„ë£Œ: {len(conversation_history)} ë©”ì‹œì§€")
            
            # ëŒ€í™” ì´ë ¥ ìˆìœ¼ë©´ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
            if memory.buffer:
                print("ëŒ€í™” ì´ë ¥ í™œìš©í•˜ì—¬ contextualized question ìƒì„±")
                contextualized_question = rewrite_chain.run(question=question)
                print(f"Rewritten question: {contextualized_question}")
                
                # ì»¨í…ìŠ¤íŠ¸ì™€ ì¬ì‘ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
                formatted_prompt = qa_prompt.format(context=context, question=contextualized_question)
            else:
                # ëŒ€í™” ì´ë ¥ ì—†ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
                formatted_prompt = qa_prompt.format(context=context, question=question)
        else:
            # ëŒ€í™” ì´ë ¥ ì—†ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
            formatted_prompt = qa_prompt.format(context=context, question=question)
        
        print(f"í¬ë§·ëœ í”„ë¡¬í”„íŠ¸: {formatted_prompt[:100]}...")
        
        # ì§ì ‘ LLM í˜¸ì¶œ
        response = llm.invoke(formatted_prompt)
        answer = response.content
        print(f"LLM ì‘ë‹µ ìˆ˜ì‹  (ê¸¸ì´: {len(answer)})")
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥
        memory.write_context({"human": question}, {"ai": answer})
        
        return answer
        
    except Exception as e:
        print(f"chatbot_with_docs ì˜¤ë¥˜: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
async def get_rag_response(question: str, candidate=None, conversation_history=None) -> ChatResponse:
    """
    RAG ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        candidate: í›„ë³´ì ì´ë¦„ ë˜ëŠ” í›„ë³´ì ê°ì²´ (ê¸°ë³¸ê°’: None)
        conversation_history: ì´ì „ ëŒ€í™” ì´ë ¥ (ì„ íƒ ì‚¬í•­)
        
    Returns:
        ChatResponse: ì±—ë´‡ ì‘ë‹µ ê°ì²´
    """
    try:
        # ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        documents, source_metadata, related_policies = search_documents(question, candidate)
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer_text = chatbot_with_docs(question, candidate, documents, conversation_history)
        
        # ê²€ìƒ‰ëœ ë©”íƒ€ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not source_metadata:
            source_name = DEFAULT_SOURCE
            if candidate:
                candidate_name = candidate.name if hasattr(candidate, 'name') else candidate
                source_name = f"{candidate_name} - {source_name}"
                
            source_metadata = SourceMetadata(
                page=0,
                source=source_name,
                creation_date=""
            )
        
        return ChatResponse(
            answer=answer_text,
            related_policies=related_policies,
            source_metadata=source_metadata
        )
    
    except Exception as e:
        return create_error_response(f"{ERROR_MESSAGE_RAG}: {str(e)}")

# ê¸°ì¡´ AI ì‘ë‹µ ìƒì„± í•¨ìˆ˜
async def get_ai_response(question: str, policies: Optional[List[Policy]] = None) -> ChatResponse:
    """
    ê¸°ì¡´ Gemini ì§ì ‘ í˜¸ì¶œ ë°©ì‹ì˜ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
    """
    if not GEMINI_API_KEY:
        return create_error_response(ERROR_MESSAGE_API_KEY)
    
    policy_context = ""
    related_policies = []
    source_metadata = None

    if policies and len(policies) > 0:
        policy_context = "ë‹¤ìŒ ì •ì±…ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:\n\n"
        
        # ì²« ë²ˆì§¸ ì •ì±…ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë³¸ ì¶œì²˜ë¡œ í™œìš©
        first_policy = policies[0]
        if hasattr(first_policy, 'meta_info') and first_policy.meta_info:
            try:
                metadata_dict = json.loads(first_policy.meta_info)
                source_metadata = SourceMetadata(
                    page=metadata_dict.get("page", 0),
                    source=metadata_dict.get("source", DEFAULT_SOURCE),
                    creation_date=metadata_dict.get("creation_date", ""),
                    source_name=metadata_dict.get("source_name", ""),
                    source_link=metadata_dict.get("source_link", ""),
                    candidate=metadata_dict.get("candidate", "")
                )
            except (json.JSONDecodeError, AttributeError, KeyError) as e:
                print(f"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        for policy in policies:
            policy_text = f"í›„ë³´: {policy.candidate.name}, ì œëª©: {policy.title}, ì¹´í…Œê³ ë¦¬: {policy.category}, ë‚´ìš©: {policy.description}"
            policy_context += policy_text + "\n\n"
            related_policies.append({
                "id": policy.id,
                "candidate_id": policy.candidate_id,
                "title": policy.title,
                "category": policy.category
            })
    
    if policy_context:
        full_prompt = f"{SYSTEM_PROMPT}\n\n{policy_context}\n\nì§ˆë¬¸: {question}"
    else:
        full_prompt = f"{SYSTEM_PROMPT}\n\nì§ˆë¬¸: {question}"

    try:
        model = genai.GenerativeModel(
            model_name=GEMINI_MODEL_NAME,
            generation_config=generation_config,
            safety_settings=safety_settings
        )

        # â±ï¸ Gemini í˜¸ì¶œì— íƒ€ì„ì•„ì›ƒ ì„¤ì •
        response = await asyncio.wait_for(
            asyncio.to_thread(model.generate_content, full_prompt),
            timeout=RESPONSE_TIMEOUT
        )

        answer_text = response.text if response.candidates and len(response.candidates) > 0 else "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ì£¼ì„¸ìš”."

        # ê¸°ë³¸ ì¶œì²˜ ì •ë³´ ì„¤ì •
        if not source_metadata:
            source_metadata = SourceMetadata(
                page=0,
                source=DEFAULT_SOURCE,
                creation_date="",
                source_name="",
                source_link="",
                candidate=""
            )

        return ChatResponse(
            answer=answer_text,
            related_policies=related_policies,
            source_metadata=source_metadata
        )

    except asyncio.TimeoutError:
        return create_error_response(ERROR_MESSAGE_TIMEOUT)
    
    except Exception as e:
        return create_error_response(f"{ERROR_MESSAGE_SERVER}: {str(e)}")